{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "41d07c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ed8347a9-f7a4-48f0-a865-46a7c433c6b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "import torch\n",
    "from torch.utils.data.sampler import Sampler\n",
    "import numpy as np\n",
    "import itertools\n",
    "import random\n",
    "\n",
    "from torchvision.datasets import SVHN, MNIST, FashionMNIST, CIFAR10, CelebA, Omniglot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8b8a736c-f24f-4aec-8fba-c7495dc0b86a",
   "metadata": {},
   "outputs": [],
   "source": [
    "root = \"/scratch/gg2501/ML_Project/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2fc182a2-8229-4577-a14e-bdc03c319fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(os.path.join(root, 'flowgmm-public'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "076dcdf3-13ad-4440-9baa-50095c92129f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import flow_ssl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "168123ec-107e-42e2-82b3-e22ae408ef4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = os.path.join(root, 'data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ff494df0-5a54-45ae-956a-5fec2b26f1fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using downloaded and verified file: /scratch/gg2501/ML_Project/data/train_32x32.mat\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "svhn_dataset = SVHN(root=data_dir, split='train', download=True)\n",
    "mnist_dataset = MNIST(root=data_dir, download=True)\n",
    "fashionmnist_dataset = FashionMNIST(root=data_dir, download=True)\n",
    "cifar_dataset = CIFAR10(root=data_dir, download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8a834da8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "from torchvision.datasets import STL10, Food101, Caltech101, GTSRB, Flowers102\n",
    "stl_dataset = STL10(root=data_dir, split='train', download=True)\n",
    "food_dataset = Food101(root=data_dir, download = True)\n",
    "# celeb_dataset = CelebA(root=data_dir, download = True)\n",
    "flowers_dataset = Flowers102(root=data_dir, download = True)\n",
    "caltech_dataset = Caltech101(root=data_dir, download = True)\n",
    "german_sign_dataset = GTSRB(root=data_dir, download = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "223f31a9-c136-4c40-bc55-220717c451ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LabeledUnlabeledBatchSampler(Sampler):\n",
    "    \"\"\"Minibatch index sampler for labeled and unlabeled indices. \n",
    "\n",
    "    An epoch is one pass through the labeled indices.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "            self, \n",
    "            labeled_idx, \n",
    "            unlabeled_idx, \n",
    "            labeled_batch_size, \n",
    "            unlabeled_batch_size):\n",
    "\n",
    "        self.labeled_idx = labeled_idx\n",
    "        self.unlabeled_idx = unlabeled_idx\n",
    "        self.unlabeled_batch_size = unlabeled_batch_size\n",
    "        self.labeled_batch_size = labeled_batch_size\n",
    "\n",
    "        assert len(self.labeled_idx) >= self.labeled_batch_size > 0\n",
    "        assert len(self.unlabeled_idx) >= self.unlabeled_batch_size > 0\n",
    "\n",
    "    @property\n",
    "    def num_labeled(self):\n",
    "        return len(self.labeled_idx)\n",
    "\n",
    "    def __iter__(self):\n",
    "        # print(\"Balle balle\")\n",
    "        labeled_iter = iterate_once(self.labeled_idx)\n",
    "        unlabeled_iter = iterate_eternally(self.unlabeled_idx)\n",
    "        return (\n",
    "            labeled_batch + unlabeled_batch\n",
    "            for (labeled_batch, unlabeled_batch)\n",
    "            in  zip(batch_iterator(labeled_iter, self.labeled_batch_size),\n",
    "                    batch_iterator(unlabeled_iter, self.unlabeled_batch_size))\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labeled_idx) // self.labeled_batch_size\n",
    "\n",
    "\n",
    "def iterate_once(iterable):\n",
    "    return np.random.permutation(iterable)\n",
    "\n",
    "\n",
    "def iterate_eternally(indices):\n",
    "    def infinite_shuffles():\n",
    "        while True:\n",
    "            yield np.random.permutation(indices)\n",
    "    return itertools.chain.from_iterable(infinite_shuffles())\n",
    "\n",
    "\n",
    "def batch_iterator(iterable, n):\n",
    "    \"Collect data into fixed-length chunks or blocks\"\n",
    "    args = [iter(iterable)] * n\n",
    "    return zip(*args)\n",
    "\n",
    "\n",
    "class TransformTwice:\n",
    "    def __init__(self, transform):\n",
    "        self.transform = transform\n",
    "\n",
    "    def __call__(self, inp):\n",
    "        out1 = self.transform(inp)\n",
    "        out2 = self.transform(inp)\n",
    "        return out1, out2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2e8d1ce8-6872-4e6e-bfd8-2531f88343d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset():\n",
    "    def __init__(self, config: dict):\n",
    "        self.data_keys = set(['mnist', 'fashionmnist', 'cifar', 'svhn'])\n",
    "        self.config = config\n",
    "        self.labeled_ids = []\n",
    "        self.unlabeled_ids = []\n",
    "        self.image_tensors = []\n",
    "        self.labels = []\n",
    "        self.counter = 0\n",
    "    \n",
    "    def prepare(self, in_data='mnist', out_data=['cifar', 'svhn', 'fashionmnist'], indata_size=5000, outdata_size=1700, label_ratio=0.02):\n",
    "        # Prepare OOD data\n",
    "        for k in out_data:\n",
    "            dataset = config[k]['dataset']\n",
    "            transforms = TransformTwice(config[k]['transforms'])\n",
    "            n_outlabels = int(outdata_size * label_ratio)\n",
    "            dataset_ids = np.random.choice(len(dataset), outdata_size)\n",
    "            dataset_labeled_ids = set(np.random.choice(dataset_ids, n_outlabels))\n",
    "            # u, c = np.unique(dataset_labeled_ids, return_counts=True)\n",
    "            # dup = u[c > 1]\n",
    "            # print(dup)\n",
    "            transform = torchvision.transforms.Compose([torchvision.transforms.PILToTensor()])\n",
    "            print(transform(dataset[0][0]).shape)\n",
    "            for idx in dataset_ids:\n",
    "                img = dataset[idx][0]\n",
    "                img_tensor = transforms(img)\n",
    "                self.image_tensors.append(img_tensor)\n",
    "                if idx in dataset_labeled_ids:\n",
    "                    self.labeled_ids.append(self.counter)\n",
    "                    self.labels.append(0)\n",
    "                else:\n",
    "                    self.unlabeled_ids.append(self.counter)\n",
    "                    self.labels.append(-1)\n",
    "                self.counter += 1\n",
    "            print(len(self.labeled_ids))\n",
    "            print(f'{k} dataset processed...')\n",
    "        \n",
    "        # Prepare ID data\n",
    "        dataset = config[in_data]['dataset']\n",
    "        transforms = TransformTwice(config[k]['transforms'])\n",
    "        n_inlabels = int(indata_size * label_ratio)\n",
    "        dataset_ids = np.random.choice(len(dataset), indata_size)\n",
    "        dataset_labeled_ids = set(np.random.choice(dataset_ids, n_inlabels))\n",
    "        for idx in dataset_ids:\n",
    "            img = dataset[idx][0]\n",
    "            img_tensor = transforms(img)\n",
    "            self.image_tensors.append(img_tensor)\n",
    "            if idx in dataset_labeled_ids:\n",
    "                self.labeled_ids.append(self.counter)\n",
    "                self.labels.append(1)\n",
    "            else:\n",
    "                self.unlabeled_ids.append(self.counter)\n",
    "                self.labels.append(-1)\n",
    "            self.counter += 1\n",
    "        print(len(self.labeled_ids))\n",
    "        print(f'{in_data} dataset processed...')\n",
    "        \n",
    "        random.shuffle(self.labeled_ids)\n",
    "        random.shuffle(self.unlabeled_ids)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_tensors)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.image_tensors[idx], self.labels[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "941c118c-c611-4633-a341-ec192018a162",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SLDataset():\n",
    "    def __init__(self, config: dict):\n",
    "        self.data_keys = set(['mnist', 'fashionmnist', 'cifar', 'svhn'])\n",
    "        self.config = config\n",
    "        self.image_tensors = []\n",
    "        self.labels = []\n",
    "    \n",
    "    def prepare(self, in_data='mnist', out_data=['cifar', 'svhn', 'fashionmnist'], indata_size=600, outdata_size=200):\n",
    "        print(out_data)\n",
    "        # Prepare OOD data\n",
    "        for k in out_data:\n",
    "            dataset = config[k]['dataset']\n",
    "            transforms = config[k]['transforms']\n",
    "            for i, (img, _) in enumerate(dataset):\n",
    "                if i == outdata_size:\n",
    "                    break\n",
    "                img_tensor = transforms(img)\n",
    "                self.image_tensors.append(img_tensor)\n",
    "            self.labels += [0] * outdata_size\n",
    "        \n",
    "        # Prepare ID data\n",
    "        dataset = config[in_data]['dataset']\n",
    "        transforms = config[in_data]['transforms']\n",
    "        for i, (img, _) in enumerate(dataset):\n",
    "            if i == indata_size:\n",
    "                break\n",
    "            img_tensor = transforms(img)\n",
    "            self.image_tensors.append(img_tensor)\n",
    "        self.labels += [1] * indata_size\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_tensors)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.image_tensors[idx], self.labels[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "84b9a022-be6a-4b52-8ffb-a43303b35f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_to_three_channel = transforms.Lambda(lambda img: img.expand(3,*img.shape[1:]))\n",
    "\n",
    "svhn_transforms = transforms.Compose([\n",
    "                    # transforms.Grayscale(),\n",
    "                    transforms.RandomCrop(32, padding=4),\n",
    "                    transforms.RandomHorizontalFlip(),\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Resize((32,32)),\n",
    "                    transform_to_three_channel,\n",
    "                ])\n",
    "\n",
    "mnist_transforms = transforms.Compose([\n",
    "                   transforms.RandomCrop(32, padding=4),\n",
    "                   transforms.RandomHorizontalFlip(),\n",
    "                   transforms.ToTensor(),\n",
    "                   transforms.Resize((32,32)),\n",
    "                   transform_to_three_channel\n",
    "                ])\n",
    "\n",
    "fashionmnist_transforms = transforms.Compose([\n",
    "                    transforms.RandomCrop(32, padding=4),\n",
    "                    transforms.RandomHorizontalFlip(),\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Resize((32,32)),\n",
    "                    transform_to_three_channel\n",
    "                ])\n",
    "\n",
    "cifar_transforms = transforms.Compose([\n",
    "                    transforms.RandomCrop(32, padding=4),\n",
    "                    transforms.RandomHorizontalFlip(),\n",
    "                    # transforms.Grayscale(),\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Resize((32,32)),\n",
    "                    transform_to_three_channel\n",
    "                ])\n",
    "\n",
    "food_transforms = transforms.Compose([\n",
    "                    transforms.RandomCrop(32, padding=4),\n",
    "                    transforms.RandomHorizontalFlip(),\n",
    "                    # transforms.Grayscale(),\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Resize((32,32)),\n",
    "                    transform_to_three_channel\n",
    "                ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "82fdbeb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "celeb_transforms = transforms.Compose([\n",
    "                    transforms.RandomCrop(32, padding=4),\n",
    "                    transforms.RandomHorizontalFlip(),\n",
    "                    # transforms.Grayscale(),\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Resize((32,32)),\n",
    "                    transform_to_three_channel\n",
    "                ])\n",
    "stl10_transforms = transforms.Compose([\n",
    "                    transforms.RandomCrop(32, padding=4),\n",
    "                    transforms.RandomHorizontalFlip(),\n",
    "                    # transforms.Grayscale(),\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Resize((32,32)),\n",
    "                    transform_to_three_channel\n",
    "                ])\n",
    "german_sign_transforms = transforms.Compose([\n",
    "                    transforms.RandomCrop(32, padding=4),\n",
    "                    transforms.RandomHorizontalFlip(),\n",
    "                    # transforms.Grayscale(),\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Resize((32,32)),\n",
    "                    transform_to_three_channel\n",
    "                ])\n",
    "caltech_transforms = transforms.Compose([\n",
    "                    transforms.RandomCrop(32, padding=4),\n",
    "                    transforms.RandomHorizontalFlip(),\n",
    "                    # transforms.Grayscale(),\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Resize((32,32)),\n",
    "                    transform_to_three_channel\n",
    "                ])\n",
    "\n",
    "flowers_transforms = transforms.Compose([\n",
    "                    transforms.RandomCrop(32, padding=4),\n",
    "                    transforms.RandomHorizontalFlip(),\n",
    "                    # transforms.Grayscale(),\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Resize((32,32)),\n",
    "                    transform_to_three_channel\n",
    "                ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f96446aa-9056-4afc-9d1a-e6a8af4eeb05",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {}\n",
    "\n",
    "config['svhn'] = {}\n",
    "config['svhn']['dataset'] = svhn_dataset\n",
    "config['svhn']['transforms'] = svhn_transforms\n",
    "\n",
    "config['mnist'] = {}\n",
    "config['mnist']['dataset'] = mnist_dataset\n",
    "config['mnist']['transforms'] = mnist_transforms\n",
    "\n",
    "config['fashionmnist'] = {}\n",
    "config['fashionmnist']['dataset'] = fashionmnist_dataset\n",
    "config['fashionmnist']['transforms'] = fashionmnist_transforms\n",
    "\n",
    "config['cifar'] = {}\n",
    "config['cifar']['dataset'] = cifar_dataset\n",
    "config['cifar']['transforms'] = cifar_transforms\n",
    "\n",
    "config['food'] = {}\n",
    "config['food']['dataset'] = food_dataset\n",
    "config['food']['transforms'] = food_transforms\n",
    "\n",
    "config['stl10'] = {}\n",
    "config['stl10']['dataset'] = stl_dataset\n",
    "config['stl10']['transforms'] = stl10_transforms\n",
    "\n",
    "config['flowers'] = {}\n",
    "config['flowers']['dataset'] = flowers_dataset\n",
    "config['flowers']['transforms'] = flowers_transforms\n",
    "\n",
    "config['caltech'] = {}\n",
    "config['caltech']['dataset'] = caltech_dataset\n",
    "config['caltech']['transforms'] = caltech_transforms\n",
    "\n",
    "config['german_sign'] = {}\n",
    "config['german_sign']['dataset'] = german_sign_dataset\n",
    "config['german_sign']['transforms'] = german_sign_transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "be1ffb39-b419-4c66-9100-0dfc81b4c611",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 28, 28])\n",
      "112\n",
      "mnist dataset processed...\n",
      "torch.Size([3, 32, 32])\n",
      "220\n",
      "svhn dataset processed...\n",
      "torch.Size([1, 28, 28])\n",
      "326\n",
      "fashionmnist dataset processed...\n",
      "torch.Size([3, 96, 96])\n",
      "525\n",
      "stl10 dataset processed...\n",
      "torch.Size([3, 512, 512])\n",
      "629\n",
      "food dataset processed...\n",
      "torch.Size([3, 337, 510])\n",
      "784\n",
      "caltech dataset processed...\n",
      "torch.Size([3, 500, 754])\n",
      "1299\n",
      "flowers dataset processed...\n",
      "torch.Size([3, 30, 29])\n",
      "1427\n",
      "german_sign dataset processed...\n",
      "2860\n",
      "cifar dataset processed...\n"
     ]
    }
   ],
   "source": [
    "ds = Dataset(config)\n",
    "ds.prepare(in_data='cifar', out_data=['mnist', 'svhn', 'fashionmnist', 'stl10','food', 'caltech','flowers','german_sign'], indata_size=40000, outdata_size=5000, label_ratio=0.02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9927f078-8ec6-4809-b273-ba677c1e043c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80000"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ds.unlabeled_ids) + len(ds.labeled_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8e6bcfaf-974b-4f0a-9b23-ddef934cad66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using downloaded and verified file: /scratch/gg2501/ML_Project/data/test_32x32.mat\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "svhn_test_dataset = SVHN(root=data_dir, split='test', download=True)\n",
    "mnist_test_dataset = MNIST(root=data_dir, train=False, download=True)\n",
    "fashionmnist_test_dataset = FashionMNIST(root=data_dir, train=False, download=True)\n",
    "cifar_test_dataset = CIFAR10(root=data_dir, train=False, download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a18390d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "stl_test_dataset = STL10(root=data_dir, split='test', download=True)\n",
    "food_test_dataset = Food101(root=data_dir, split='test', download = True)\n",
    "# celeb_dataset = CelebA(root=data_dir, download = True)\n",
    "flowers_test_dataset = Flowers102(root=data_dir, split='test', download = True)\n",
    "caltech_test_dataset = Caltech101(root=data_dir, download = True)\n",
    "german_sign_test_dataset = GTSRB(root=data_dir, split='test', download = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c460804b-f229-49d3-b362-68948025a1e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_config = {}\n",
    "\n",
    "test_config['svhn'] = {}\n",
    "test_config['svhn']['dataset'] = svhn_test_dataset\n",
    "test_config['svhn']['transforms'] = svhn_transforms\n",
    "\n",
    "test_config['mnist'] = {}\n",
    "test_config['mnist']['dataset'] = mnist_test_dataset\n",
    "test_config['mnist']['transforms'] = mnist_transforms\n",
    "\n",
    "test_config['fashionmnist'] = {}\n",
    "test_config['fashionmnist']['dataset'] = fashionmnist_test_dataset\n",
    "test_config['fashionmnist']['transforms'] = fashionmnist_transforms\n",
    "\n",
    "test_config['cifar'] = {}\n",
    "test_config['cifar']['dataset'] = cifar_test_dataset\n",
    "test_config['cifar']['transforms'] = cifar_transforms\n",
    "\n",
    "test_config['food'] = {}\n",
    "test_config['food']['dataset'] = food_test_dataset\n",
    "test_config['food']['transforms'] = food_transforms\n",
    "\n",
    "test_config['stl10'] = {}\n",
    "test_config['stl10']['dataset'] = stl_test_dataset\n",
    "test_config['stl10']['transforms'] = stl10_transforms\n",
    "\n",
    "test_config['flowers'] = {}\n",
    "test_config['flowers']['dataset'] = flowers_test_dataset\n",
    "test_config['flowers']['transforms'] = flowers_transforms\n",
    "\n",
    "test_config['caltech'] = {}\n",
    "test_config['caltech']['dataset'] = caltech_test_dataset\n",
    "test_config['caltech']['transforms'] = caltech_transforms\n",
    "\n",
    "test_config['german_sign'] = {}\n",
    "test_config['german_sign']['dataset'] = german_sign_test_dataset\n",
    "test_config['german_sign']['transforms'] = german_sign_transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "da501a38-eef3-4564-add8-434a2fdbc2aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['mnist', 'svhn', 'fashionmnist', 'stl10', 'food', 'caltech', 'flowers', 'german_sign']\n"
     ]
    }
   ],
   "source": [
    "test_ds = SLDataset(test_config)\n",
    "test_ds.prepare(in_data='cifar', out_data=['mnist', 'svhn', 'fashionmnist', 'stl10','food', 'caltech','flowers','german_sign'], indata_size=2000, outdata_size=250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cce47f49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4000"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2a8aea62-a23d-4b3c-8d6b-50f7a2f4b083",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batch_sampler = LabeledUnlabeledBatchSampler(ds.labeled_ids, ds.unlabeled_ids, 32, 32)\n",
    "trainloader = torch.utils.data.DataLoader(ds, batch_sampler=train_batch_sampler, pin_memory=True)\n",
    "testloader = torch.utils.data.DataLoader(test_ds, batch_size=64, shuffle=True, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f553a657-6dc7-49ba-b50a-05ae687af69a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "for batch in trainloader:\n",
    "    # print(len(batch))\n",
    "    print(batch[0][0].shape)\n",
    "    print(batch[0][1].shape)\n",
    "    print(batch[1].shape)\n",
    "    # cv2.imshow('image window1', batch[0][0])\n",
    "    # cv2.imshow('image window2', batch[1][0])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "524191b2-a35e-48d9-aafa-9bed0e5f0f18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "-1\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "for batch in testloader:\n",
    "    print(len(batch))\n",
    "    print(batch[0].get_device())\n",
    "    print(batch[0].shape)\n",
    "    print(batch[1].shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "508e39ed-2a8c-45eb-95a5-8d4e483539e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "cls_num_list = [6000, 45000]\n",
    "\n",
    "class LDAMLoss(nn.Module):\n",
    "    \n",
    "    def __init__(self, cls_num_list, max_m=0.5, weight=None, s=30):\n",
    "        super(LDAMLoss, self).__init__()\n",
    "        m_list = 1.0 / np.sqrt(np.sqrt(cls_num_list))\n",
    "        m_list = m_list * (max_m / np.max(m_list))\n",
    "        m_list = torch.cuda.FloatTensor(m_list)\n",
    "        self.m_list = m_list\n",
    "        assert s > 0\n",
    "        self.s = s\n",
    "        self.weight = weight\n",
    "\n",
    "    def forward(self, x, target):\n",
    "        index = torch.zeros_like(x, dtype=torch.uint8)\n",
    "        index.scatter_(1, target.data.view(-1, 1), 1)\n",
    "        \n",
    "        index_float = index.type(torch.cuda.FloatTensor)\n",
    "        batch_m = torch.matmul(self.m_list[None, :], index_float.transpose(0,1))\n",
    "        batch_m = batch_m.view((-1, 1))\n",
    "        x_m = x - batch_m\n",
    "    \n",
    "        output = torch.where(index, x_m, x)\n",
    "        return F.cross_entropy(self.s*output, target, weight=self.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2124dd42-191d-454d-b2bb-51705c0466f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_shape = (3, 32, 32)\n",
    "flow = 'ResidualFlow'\n",
    "model_cfg = getattr(flow_ssl, flow)\n",
    "net = model_cfg(in_channels=img_shape[0], num_classes=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "65002dfb-566c-4820-add7-8ffcf1bf0869",
   "metadata": {},
   "outputs": [],
   "source": [
    "if flow in [\"iCNN3d\", \"iResnetProper\",\"SmallResidualFlow\",\"ResidualFlow\",\"MNISTResidualFlow\"]:\n",
    "    net = net.flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f5920588-44f1-46f1-90ce-575b5529cc7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from experiments.train_flows.utils import train_utils\n",
    "\n",
    "means = 'random'\n",
    "means_r = 1.0\n",
    "cov_std = 1.0\n",
    "# img_shape = (1, 32, 32)\n",
    "device = 'cuda'\n",
    "n_classes = 2\n",
    "\n",
    "net = net.to(device)\n",
    "r = means_r\n",
    "cov_std = torch.ones((n_classes)) * cov_std\n",
    "cov_std = cov_std.to(device)\n",
    "means = train_utils.get_means(means, num_means=n_classes, r=means_r, trainloader=trainloader, \n",
    "                        shape=img_shape, device=device, net=net)\n",
    "means_init = means.clone().detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9487a807-bb50-4650-b26f-e294679e7af0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Means: tensor([[-0.5294,  0.7078, -0.9747,  ...,  0.2047, -0.2061, -2.2594],\n",
      "        [-0.3701, -0.8483,  2.1468,  ..., -0.8181,  0.4732,  1.2964]],\n",
      "       device='cuda:0')\n",
      "Cov std: tensor([1., 1.], device='cuda:0')\n",
      "Pairwise dists: [[ 0.         76.90869991]\n",
      " [76.90869991  0.        ]]\n"
     ]
    }
   ],
   "source": [
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "print(\"Means:\", means)\n",
    "print(\"Cov std:\", cov_std)\n",
    "means_np = means.cpu().numpy()\n",
    "print(\"Pairwise dists:\", cdist(means_np, means_np))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "dd372806-894b-4d04-bac1-9266b46015fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using learnable means\n"
     ]
    }
   ],
   "source": [
    "from flow_ssl.distributions import SSLGaussMixture\n",
    "from flow_ssl import FlowLoss\n",
    "\n",
    "means_trainable = True\n",
    "covs_trainable = True\n",
    "weights_trainable = True\n",
    "\n",
    "if means_trainable:\n",
    "    print(\"Using learnable means\")\n",
    "    means = torch.tensor(means_np, requires_grad=True, device=device)\n",
    "\n",
    "prior = SSLGaussMixture(means, device=device)\n",
    "prior.weights.requires_grad = weights_trainable\n",
    "prior.inv_cov_stds.requires_grad = covs_trainable\n",
    "loss_fn = FlowLoss(prior)\n",
    "sup_loss_fn = LDAMLoss(cls_num_list, s=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "416efc52-ebbf-4011-8055-15feb3e81734",
   "metadata": {},
   "outputs": [],
   "source": [
    "from experiments.train_flows.utils import norm_util\n",
    "import torch.optim as optim\n",
    "\n",
    "param_groups = norm_util.get_param_groups(net, 0.0, norm_suffix='weight_g')\n",
    "\n",
    "optimizer = optim.Adam(param_groups, lr=5e-4, weight_decay=1e-2)\n",
    "opt_gmm = optim.Adam([prior.means, prior.weights, prior.inv_cov_stds], lr=1e-4, weight_decay=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "11c3f709-4854-4f51-95c1-9d0ab84a5ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "writer = SummaryWriter(log_dir='./')\n",
    "device = 'cuda' if torch.cuda.is_available() and len([0]) > 0 else 'cpu'\n",
    "start_epoch = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "85e9c466-b4f6-4f86-a329-455280c72332",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_classifier(epoch, net, testloader, device, loss_fn, writer=None, postfix=\"\",\n",
    "                    show_classification_images=False, confusion=False):\n",
    "    net.eval()\n",
    "    loss_meter = shell_util.AverageMeter()\n",
    "    jaclogdet_meter = shell_util.AverageMeter()\n",
    "    acc_meter = shell_util.AverageMeter()\n",
    "    all_pred_labels = []\n",
    "    all_xs = []\n",
    "    all_ys = []\n",
    "    all_zs = []\n",
    "    with torch.no_grad():\n",
    "        with tqdm(total=len(testloader.dataset)) as progress_bar:\n",
    "            for x, y in testloader:\n",
    "                all_xs.append(x.data.numpy())\n",
    "                all_ys.append(y.data.numpy())\n",
    "                x = x.to(device)\n",
    "                y = y.to(device)\n",
    "                z = net(x)\n",
    "                all_zs.append(z.cpu().data.numpy())\n",
    "                sldj = net.logdet()\n",
    "                loss = loss_fn(z, y=y, sldj=sldj)\n",
    "                loss_meter.update(loss.item(), x.size(0))\n",
    "                jaclogdet_meter.update(sldj.mean().item(), x.size(0))\n",
    "\n",
    "                preds = loss_fn.prior.classify(z.reshape((len(z), -1)))\n",
    "                preds = preds.reshape(y.shape)\n",
    "                all_pred_labels.append(preds.cpu().data.numpy())\n",
    "                acc = (preds == y).float().mean().item()\n",
    "                acc_meter.update(acc, x.size(0))\n",
    "\n",
    "                progress_bar.set_postfix(loss=loss_meter.avg,\n",
    "                                     bpd=optim_util.bits_per_dim(x, loss_meter.avg),\n",
    "                                     acc=acc_meter.avg)\n",
    "                progress_bar.update(x.size(0))\n",
    "    all_pred_labels = np.hstack(all_pred_labels)\n",
    "    all_xs = np.vstack(all_xs)\n",
    "    all_zs = np.vstack(all_zs)\n",
    "    all_ys = np.hstack(all_ys)\n",
    "\n",
    "    if writer is not None:\n",
    "        writer.add_scalar(\"test/loss{}\".format(postfix), loss_meter.avg, epoch)\n",
    "        writer.add_scalar(\"test/acc{}\".format(postfix), acc_meter.avg, epoch)\n",
    "        writer.add_scalar(\"test/bpd{}\".format(postfix), optim_util.bits_per_dim(x, loss_meter.avg), epoch)\n",
    "        writer.add_scalar(\"test/jaclogdet{}\".format(postfix), jaclogdet_meter.avg, epoch)\n",
    "\n",
    "        for cls in range(np.max(all_pred_labels)+1):\n",
    "            num_imgs_cls = (all_pred_labels==cls).sum()\n",
    "            writer.add_scalar(\"test_clustering/num_class_{}_{}\".format(cls,postfix), \n",
    "                    num_imgs_cls, epoch)\n",
    "            if num_imgs_cls == 0:\n",
    "                writer.add_scalar(\"test_clustering/num_class_{}_{}\".format(cls,postfix), \n",
    "                    0., epoch)\n",
    "                continue\n",
    "            writer.add_histogram('label_distributions/num_class_{}_{}'.format(cls,postfix), \n",
    "                    all_ys[all_pred_labels==cls], epoch)\n",
    "\n",
    "            writer.add_histogram(\n",
    "                'distance_distributions/num_class_{}'.format(cls),\n",
    "                torch.norm(torch.tensor(all_zs[all_pred_labels==cls]) - loss_fn.prior.means[cls].cpu(), p=2, dim=1),\n",
    "                epoch\n",
    "            )\n",
    "\n",
    "            if show_classification_images:\n",
    "                images_cls = all_xs[all_pred_labels==cls][:10]\n",
    "                images_cls = torch.from_numpy(images_cls).float()\n",
    "                images_cls_concat = torchvision.utils.make_grid(\n",
    "                        images_cls, nrow=2, padding=2, pad_value=255)\n",
    "                writer.add_image(\"test_clustering/class_{}\".format(cls), \n",
    "                        images_cls_concat)\n",
    "\n",
    "        if confusion:\n",
    "            fig = plt.figure(figsize=(8, 8))\n",
    "            cm = confusion_matrix(all_ys, all_pred_labels)\n",
    "            cm = np.around(cm.astype('float') / cm.sum(axis=1)[:, np.newaxis], decimals=2)\n",
    "            sns.heatmap(cm, annot=True, cmap=plt.cm.Blues)\n",
    "            plt.ylabel('True label')\n",
    "            plt.xlabel('Predicted label')\n",
    "            conf_img = torch.tensor(np.fromstring(fig.canvas.tostring_rgb(), dtype=np.uint8, sep=''))\n",
    "            conf_img = torch.tensor(conf_img.reshape(fig.canvas.get_width_height()[::-1] + (3,))).transpose(0, 2).transpose(1, 2)\n",
    "            writer.add_image(\"confusion\", conf_img, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c5aebe9b-7dd6-4e51-8ae8-24ce144978cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_labels = len(ds.labeled_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9e14cd2d-44e1-4f81-8ba5-b868870dc176",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(net, prior, batch_size, cls, device, sample_shape):\n",
    "    \"\"\"Sample from RealNVP model.\n",
    "    Args:\n",
    "        net (torch.nn.DataParallel): The RealNVP model wrapped in DataParallel.\n",
    "        batch_size (int): Number of samples to generate.\n",
    "        device (torch.device): Device to use.\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        if cls is not None:\n",
    "            z = prior.sample((batch_size,), gaussian_id=cls)\n",
    "        else:\n",
    "            z = prior.sample((batch_size,))\n",
    "        x = net.inverse(z)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c129e139-d9c4-48b9-8d47-cd070b329153",
   "metadata": {},
   "outputs": [],
   "source": [
    "from experiments.train_flows.utils import optim_util\n",
    "\n",
    "def train(epoch, net, trainloader, device, optimizer, opt_gmm, loss_fn,\n",
    "          label_weight, max_grad_norm, consistency_weight,\n",
    "          writer, sup_loss_fn, use_unlab=True,  acc_train_all_labels=False,\n",
    "          ):\n",
    "    print('\\nEpoch: %d' % epoch)\n",
    "    net.train()\n",
    "    loss_meter = shell_util.AverageMeter()\n",
    "    loss_unsup_meter = shell_util.AverageMeter()\n",
    "    loss_nll_meter = shell_util.AverageMeter()\n",
    "    loss_consistency_meter = shell_util.AverageMeter()\n",
    "    jaclogdet_meter = shell_util.AverageMeter()\n",
    "    acc_meter = shell_util.AverageMeter()\n",
    "    acc_all_meter = shell_util.AverageMeter()\n",
    "    with tqdm(total=2*total_labels) as progress_bar:\n",
    "        for (x1, x2), y in trainloader:\n",
    "\n",
    "            x1 = x1.to(device)\n",
    "            if not acc_train_all_labels:\n",
    "                y = y.to(device)\n",
    "            else:\n",
    "                y, y_all_lab = y[:, 0], y[:, 1]\n",
    "                y = y.to(device)\n",
    "                y_all_lab = y_all_lab.to(device)\n",
    "\n",
    "            labeled_mask = (y != NO_LABEL)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            opt_gmm.zero_grad()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                x2 = x2.to(device)\n",
    "                # z21 = net(x2)\n",
    "                # z22 = net(transform(x2))\n",
    "                # y22 = classify(z22)\n",
    "                # l_cons = FlowLoss(z21, y22)\n",
    "                # print(x2.get_device(), next(net.parameters()).is_cuda)\n",
    "                z2 = net(x2)\n",
    "                z2 = z2.detach()\n",
    "                pred2 = loss_fn.prior.classify(z2.reshape((len(z2), -1)))\n",
    "\n",
    "            z1 = net(x1)\n",
    "            sldj = net.logdet()\n",
    "\n",
    "            z_all = z1.reshape((len(z1), -1))\n",
    "            z_labeled = z_all[labeled_mask]\n",
    "            y_labeled = y[labeled_mask]\n",
    "\n",
    "            logits_all = loss_fn.prior.class_logits(z_all)\n",
    "            logits_labeled = logits_all[labeled_mask]\n",
    "            loss_nll = F.cross_entropy(logits_labeled, y_labeled)\n",
    "            # loss_nll = sup_loss_fn(logits_labeled, y_labeled)\n",
    "            # print(loss_nll)\n",
    "            # loss_nll = loss_nll.mean()\n",
    "            # print(loss_nll)\n",
    "\n",
    "            if use_unlab:\n",
    "                loss_unsup = loss_fn(z1, sldj=sldj)\n",
    "                # print(loss_unsup)\n",
    "                loss = loss_nll * label_weight + loss_unsup\n",
    "            else:\n",
    "                loss_unsup = torch.tensor([0.])\n",
    "                loss = loss_nll\n",
    "\n",
    "            # consistency loss\n",
    "            loss_consistency = loss_fn(z1, sldj=sldj, y=pred2)\n",
    "            loss = loss + loss_consistency * consistency_weight\n",
    "\n",
    "            loss.backward()\n",
    "            optim_util.clip_grad_norm(optimizer, max_grad_norm)\n",
    "            optimizer.step()\n",
    "            opt_gmm.step()\n",
    "\n",
    "            preds_all = torch.argmax(logits_all, dim=1)\n",
    "            preds = preds_all[labeled_mask]\n",
    "            acc = (preds == y_labeled).float().mean().item()\n",
    "            if acc_train_all_labels:\n",
    "                acc_all = (preds_all == y_all_lab).float().mean().item()\n",
    "            else:\n",
    "                acc_all = acc\n",
    "\n",
    "            acc_meter.update(acc, x1.size(0))\n",
    "            acc_all_meter.update(acc_all, x1.size(0))\n",
    "            loss_meter.update(loss.item(), x1.size(0))\n",
    "            loss_unsup_meter.update(loss_unsup.item(), x1.size(0))\n",
    "            loss_nll_meter.update(loss_nll.item(), x1.size(0))\n",
    "            jaclogdet_meter.update(sldj.mean().item(), x1.size(0))\n",
    "            loss_consistency_meter.update(loss_consistency.item(), x1.size(0))\n",
    "\n",
    "            progress_bar.set_postfix(loss=loss_meter.avg,\n",
    "                                     bpd=optim_util.bits_per_dim(x1, loss_unsup_meter.avg),\n",
    "                                     acc=acc_meter.avg,\n",
    "                                     acc_all=acc_all_meter.avg)\n",
    "            progress_bar.update(y_labeled.size(0))\n",
    "\n",
    "    x1_img = torchvision.utils.make_grid(x1[:10], nrow=2 , padding=2, pad_value=255)\n",
    "    x2_img = torchvision.utils.make_grid(x2[:10], nrow=2 , padding=2, pad_value=255)\n",
    "    writer.add_image(\"data/x1\", x1_img)\n",
    "    writer.add_image(\"data/x2\", x2_img)\n",
    "\n",
    "    writer.add_scalar(\"train/loss\", loss_meter.avg, epoch)\n",
    "    writer.add_scalar(\"train/loss_unsup\", loss_unsup_meter.avg, epoch)\n",
    "    writer.add_scalar(\"train/loss_nll\", loss_nll_meter.avg, epoch)\n",
    "    writer.add_scalar(\"train/jaclogdet\", jaclogdet_meter.avg, epoch)\n",
    "    writer.add_scalar(\"train/acc\", acc_meter.avg, epoch)\n",
    "    writer.add_scalar(\"train/acc_all\", acc_all_meter.avg, epoch)\n",
    "    writer.add_scalar(\"train/bpd\", optim_util.bits_per_dim(x1, loss_unsup_meter.avg), epoch)\n",
    "    writer.add_scalar(\"train/loss_consistency\", loss_consistency_meter.avg, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "60bb4663-d856-41b4-8e60-b657bba607bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a88ffc7686842f2bfa3e4412ad686a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5720 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/gg2501/ML_Project/flowgmm-public/flow_ssl/distributions.py:62: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  log_probs_weighted = log_probs + torch.log(F.softmax(self.weights))\n",
      "/scratch/gg2501/ML_Project/flowgmm-public/flow_ssl/distributions.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  mixture_log_probs = torch.logsumexp(all_log_probs + torch.log(F.softmax(self.weights)), dim=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ff8aff271d54c728619464e5f4b9eb5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/state/partition1/job-28042615/ipykernel_89665/1258797901.py:78: DeprecationWarning: The binary mode of fromstring is deprecated, as it behaves surprisingly on unicode inputs. Use frombuffer instead\n",
      "  conf_img = torch.tensor(np.fromstring(fig.canvas.tostring_rgb(), dtype=np.uint8, sep=''))\n",
      "/state/partition1/job-28042615/ipykernel_89665/1258797901.py:79: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  conf_img = torch.tensor(conf_img.reshape(fig.canvas.get_width_height()[::-1] + (3,))).transpose(0, 2).transpose(1, 2)\n",
      "/state/partition1/job-28042615/ipykernel_89665/3055084958.py:74: DeprecationWarning: The binary mode of fromstring is deprecated, as it behaves surprisingly on unicode inputs. Use frombuffer instead\n",
      "  img_data = torch.tensor(np.fromstring(fig.canvas.tostring_rgb(), dtype=np.uint8, sep=''))\n",
      "/state/partition1/job-28042615/ipykernel_89665/3055084958.py:75: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  img_data = torch.tensor(img_data.reshape(fig.canvas.get_width_height()[::-1] + (3,))).transpose(0, 2).transpose(1, 2)\n",
      "/state/partition1/job-28042615/ipykernel_89665/3055084958.py:79: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  writer.add_scalar(\"train_gmm/weight/{}\".format(i), F.softmax(prior.weights)[i], epoch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1e4b882783240b89ce2bb4dc868f237",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5720 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/gg2501/ML_Project/flowgmm-public/flow_ssl/distributions.py:62: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  log_probs_weighted = log_probs + torch.log(F.softmax(self.weights))\n",
      "/scratch/gg2501/ML_Project/flowgmm-public/flow_ssl/distributions.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  mixture_log_probs = torch.logsumexp(all_log_probs + torch.log(F.softmax(self.weights)), dim=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f75ab510b074f72a7ccf34403ee9560",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5720 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71b49e7c2ce64a04bfd12df788eac89d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/state/partition1/job-28042615/ipykernel_89665/1258797901.py:78: DeprecationWarning: The binary mode of fromstring is deprecated, as it behaves surprisingly on unicode inputs. Use frombuffer instead\n",
      "  conf_img = torch.tensor(np.fromstring(fig.canvas.tostring_rgb(), dtype=np.uint8, sep=''))\n",
      "/state/partition1/job-28042615/ipykernel_89665/1258797901.py:79: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  conf_img = torch.tensor(conf_img.reshape(fig.canvas.get_width_height()[::-1] + (3,))).transpose(0, 2).transpose(1, 2)\n",
      "/state/partition1/job-28042615/ipykernel_89665/3055084958.py:74: DeprecationWarning: The binary mode of fromstring is deprecated, as it behaves surprisingly on unicode inputs. Use frombuffer instead\n",
      "  img_data = torch.tensor(np.fromstring(fig.canvas.tostring_rgb(), dtype=np.uint8, sep=''))\n",
      "/state/partition1/job-28042615/ipykernel_89665/3055084958.py:75: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  img_data = torch.tensor(img_data.reshape(fig.canvas.get_width_height()[::-1] + (3,))).transpose(0, 2).transpose(1, 2)\n",
      "/state/partition1/job-28042615/ipykernel_89665/3055084958.py:79: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  writer.add_scalar(\"train_gmm/weight/{}\".format(i), F.softmax(prior.weights)[i], epoch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42f13f1fd2d04370a6d96a636e82c387",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5720 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/gg2501/ML_Project/flowgmm-public/flow_ssl/distributions.py:62: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  log_probs_weighted = log_probs + torch.log(F.softmax(self.weights))\n",
      "/scratch/gg2501/ML_Project/flowgmm-public/flow_ssl/distributions.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  mixture_log_probs = torch.logsumexp(all_log_probs + torch.log(F.softmax(self.weights)), dim=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 4\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b943b0f1aba475eaaca178508a9a766",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5720 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9004ca85b0eb45b1bd17bc1afb88821a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/state/partition1/job-28042615/ipykernel_89665/1258797901.py:78: DeprecationWarning: The binary mode of fromstring is deprecated, as it behaves surprisingly on unicode inputs. Use frombuffer instead\n",
      "  conf_img = torch.tensor(np.fromstring(fig.canvas.tostring_rgb(), dtype=np.uint8, sep=''))\n",
      "/state/partition1/job-28042615/ipykernel_89665/1258797901.py:79: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  conf_img = torch.tensor(conf_img.reshape(fig.canvas.get_width_height()[::-1] + (3,))).transpose(0, 2).transpose(1, 2)\n",
      "/state/partition1/job-28042615/ipykernel_89665/3055084958.py:74: DeprecationWarning: The binary mode of fromstring is deprecated, as it behaves surprisingly on unicode inputs. Use frombuffer instead\n",
      "  img_data = torch.tensor(np.fromstring(fig.canvas.tostring_rgb(), dtype=np.uint8, sep=''))\n",
      "/state/partition1/job-28042615/ipykernel_89665/3055084958.py:75: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  img_data = torch.tensor(img_data.reshape(fig.canvas.get_width_height()[::-1] + (3,))).transpose(0, 2).transpose(1, 2)\n",
      "/state/partition1/job-28042615/ipykernel_89665/3055084958.py:79: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  writer.add_scalar(\"train_gmm/weight/{}\".format(i), F.softmax(prior.weights)[i], epoch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68f19b6ebdea443b93b381dc13b5b3c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5720 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/gg2501/ML_Project/flowgmm-public/flow_ssl/distributions.py:62: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  log_probs_weighted = log_probs + torch.log(F.softmax(self.weights))\n",
      "/scratch/gg2501/ML_Project/flowgmm-public/flow_ssl/distributions.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  mixture_log_probs = torch.logsumexp(all_log_probs + torch.log(F.softmax(self.weights)), dim=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 6\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a69a354494884ee2be51f87322bd35d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5720 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc3cdb9369d24b62a06171e9171149fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/state/partition1/job-28042615/ipykernel_89665/1258797901.py:78: DeprecationWarning: The binary mode of fromstring is deprecated, as it behaves surprisingly on unicode inputs. Use frombuffer instead\n",
      "  conf_img = torch.tensor(np.fromstring(fig.canvas.tostring_rgb(), dtype=np.uint8, sep=''))\n",
      "/state/partition1/job-28042615/ipykernel_89665/1258797901.py:79: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  conf_img = torch.tensor(conf_img.reshape(fig.canvas.get_width_height()[::-1] + (3,))).transpose(0, 2).transpose(1, 2)\n",
      "/state/partition1/job-28042615/ipykernel_89665/3055084958.py:74: DeprecationWarning: The binary mode of fromstring is deprecated, as it behaves surprisingly on unicode inputs. Use frombuffer instead\n",
      "  img_data = torch.tensor(np.fromstring(fig.canvas.tostring_rgb(), dtype=np.uint8, sep=''))\n",
      "/state/partition1/job-28042615/ipykernel_89665/3055084958.py:75: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  img_data = torch.tensor(img_data.reshape(fig.canvas.get_width_height()[::-1] + (3,))).transpose(0, 2).transpose(1, 2)\n",
      "/state/partition1/job-28042615/ipykernel_89665/3055084958.py:79: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  writer.add_scalar(\"train_gmm/weight/{}\".format(i), F.softmax(prior.weights)[i], epoch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 7\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37113c31d7084fcd882ce50d6e0128e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5720 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/gg2501/ML_Project/flowgmm-public/flow_ssl/distributions.py:62: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  log_probs_weighted = log_probs + torch.log(F.softmax(self.weights))\n",
      "/scratch/gg2501/ML_Project/flowgmm-public/flow_ssl/distributions.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  mixture_log_probs = torch.logsumexp(all_log_probs + torch.log(F.softmax(self.weights)), dim=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 8\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d21e8757f874486c82210187b93349ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5720 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d2001f8d42d47b681510eb0d7aa40c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/state/partition1/job-28042615/ipykernel_89665/1258797901.py:78: DeprecationWarning: The binary mode of fromstring is deprecated, as it behaves surprisingly on unicode inputs. Use frombuffer instead\n",
      "  conf_img = torch.tensor(np.fromstring(fig.canvas.tostring_rgb(), dtype=np.uint8, sep=''))\n",
      "/state/partition1/job-28042615/ipykernel_89665/1258797901.py:79: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  conf_img = torch.tensor(conf_img.reshape(fig.canvas.get_width_height()[::-1] + (3,))).transpose(0, 2).transpose(1, 2)\n",
      "/state/partition1/job-28042615/ipykernel_89665/3055084958.py:74: DeprecationWarning: The binary mode of fromstring is deprecated, as it behaves surprisingly on unicode inputs. Use frombuffer instead\n",
      "  img_data = torch.tensor(np.fromstring(fig.canvas.tostring_rgb(), dtype=np.uint8, sep=''))\n",
      "/state/partition1/job-28042615/ipykernel_89665/3055084958.py:75: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  img_data = torch.tensor(img_data.reshape(fig.canvas.get_width_height()[::-1] + (3,))).transpose(0, 2).transpose(1, 2)\n",
      "/state/partition1/job-28042615/ipykernel_89665/3055084958.py:79: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  writer.add_scalar(\"train_gmm/weight/{}\".format(i), F.softmax(prior.weights)[i], epoch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 9\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d85f52f0c814fbbb3350ef811314e1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5720 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/gg2501/ML_Project/flowgmm-public/flow_ssl/distributions.py:62: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  log_probs_weighted = log_probs + torch.log(F.softmax(self.weights))\n",
      "/scratch/gg2501/ML_Project/flowgmm-public/flow_ssl/distributions.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  mixture_log_probs = torch.logsumexp(all_log_probs + torch.log(F.softmax(self.weights)), dim=1)\n"
     ]
    }
   ],
   "source": [
    "from experiments.train_flows.utils import shell_util\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "import torch.nn as nn\n",
    "import math\n",
    "import torch.nn.init as init\n",
    "import torch.nn.functional as F\n",
    "\n",
    "NO_LABEL = -1\n",
    "schedule = None\n",
    "n_epochs = 10\n",
    "lr = 5e-4\n",
    "lr_gmm = 1e-4\n",
    "consistency_weight = 0.01\n",
    "consistency_rampup = 1\n",
    "label_weight = 1.0\n",
    "max_grad_norm = 100.0\n",
    "save_freq = 2\n",
    "ckptdir = './'\n",
    "eval_freq = 2\n",
    "confusion = True\n",
    "num_samples = 50\n",
    "\n",
    "def linear_rampup(final_value, epoch, num_epochs, start_epoch=0):\n",
    "    t = (epoch - start_epoch + 1) / num_epochs\n",
    "    if t > 1:\n",
    "        t = 1.\n",
    "    return t * final_value\n",
    "\n",
    "for epoch in range(start_epoch, n_epochs):\n",
    "    cons_weight = linear_rampup(consistency_weight, epoch, consistency_rampup, start_epoch)\n",
    "    \n",
    "    writer.add_scalar(\"hypers/learning_rate\", lr, epoch)\n",
    "    writer.add_scalar(\"hypers/learning_rate_gmm\", lr_gmm, epoch)\n",
    "    writer.add_scalar(\"hypers/consistency_weight\", cons_weight, epoch)\n",
    "\n",
    "    train(epoch, net, trainloader, device, optimizer, opt_gmm, loss_fn,\n",
    "          label_weight, max_grad_norm, cons_weight,\n",
    "          writer, sup_loss_fn, use_unlab=True)\n",
    "\n",
    "    # Save checkpoint\n",
    "    if (epoch % save_freq == 0):\n",
    "        print('Saving...')\n",
    "        state = {\n",
    "            'net': net.state_dict(),\n",
    "            'epoch': epoch,\n",
    "            'means': prior.means,\n",
    "        }\n",
    "        os.makedirs(ckptdir, exist_ok=True)\n",
    "        torch.save(state, os.path.join(ckptdir, str(epoch)+'.pt'))\n",
    "\n",
    "    # Save samples and data\n",
    "    if epoch % eval_freq == 0:\n",
    "        test_classifier(epoch, net, testloader, device, loss_fn, writer, confusion=confusion)\n",
    "        # if args.swa:\n",
    "        #     optimizer.swap_swa_sgd() \n",
    "        #     print(\"updating bn\")\n",
    "        #     SWA.bn_update(bn_loader, net)\n",
    "        #     utils.test_classifier(epoch, net, testloader, device, loss_fn, \n",
    "        #             writer, postfix=\"_swa\")\n",
    "\n",
    "        z_means = prior.means\n",
    "        data_means = net.inverse(z_means)\n",
    "        z_mean_imgs = torchvision.utils.make_grid(\n",
    "                z_means.reshape((n_classes, *img_shape)), nrow=2)\n",
    "        data_mean_imgs = torchvision.utils.make_grid(\n",
    "                data_means.reshape((n_classes, *img_shape)), nrow=2)\n",
    "        writer.add_image(\"z_means\", z_mean_imgs, epoch)\n",
    "        writer.add_image(\"data_means\", data_mean_imgs, epoch)\n",
    "\n",
    "        means_np = prior.means.detach().cpu().numpy()\n",
    "        fig = plt.figure(figsize=(8, 8))\n",
    "        sns.heatmap(cdist(means_np, means_np))\n",
    "        img_data = torch.tensor(np.fromstring(fig.canvas.tostring_rgb(), dtype=np.uint8, sep=''))\n",
    "        img_data = torch.tensor(img_data.reshape(fig.canvas.get_width_height()[::-1] + (3,))).transpose(0, 2).transpose(1, 2)\n",
    "        writer.add_image(\"mean_dists\", img_data, epoch)\n",
    "\n",
    "        for i in range(n_classes):\n",
    "            writer.add_scalar(\"train_gmm/weight/{}\".format(i), F.softmax(prior.weights)[i], epoch)\n",
    "\n",
    "        for i in range(n_classes):\n",
    "            writer.add_scalar(\"train_gmm/cov/{}\".format(i), F.softplus(prior.inv_cov_stds[i])**2, epoch)\n",
    "\n",
    "        for i in range(n_classes):\n",
    "            writer.add_scalar(\"train_gmm/mean_dist_init/{}\".format(i), torch.norm(prior.means[i]-means_init[i], 2), epoch)\n",
    "\n",
    "        images = []\n",
    "        for i in range(n_classes):\n",
    "            images_cls = sample(net, loss_fn.prior, num_samples // n_classes,\n",
    "                                      cls=i, device=device, sample_shape=img_shape)\n",
    "            images.append(images_cls)\n",
    "            images_cls_concat = torchvision.utils.make_grid(\n",
    "                    images_cls, nrow=2, padding=2, pad_value=255)\n",
    "            writer.add_image(\"samples/class_\"+str(i), images_cls_concat)\n",
    "        images = torch.cat(images)\n",
    "        os.makedirs(os.path.join(ckptdir, 'samples'), exist_ok=True)\n",
    "        images_concat = torchvision.utils.make_grid(images, nrow=num_samples //  n_classes , padding=2, pad_value=255)\n",
    "        os.makedirs(ckptdir, exist_ok=True)\n",
    "        torchvision.utils.save_image(images_concat, \n",
    "                                    os.path.join(ckptdir, 'samples/epoch_{}.png'.format(epoch)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4677551-9a12-4364-abda-5e4931a608b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(ds.labeled_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d17aae14-9f2f-4f72-a39f-1e6c61f9369e",
   "metadata": {},
   "outputs": [],
   "source": [
    "resume = './4.pt'\n",
    "print('Resuming from checkpoint at', resume)\n",
    "checkpoint = torch.load(resume)\n",
    "# net.load_state_dict(checkpoint['net'])\n",
    "# start_epoch = checkpoint['epoch']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "467d4f02-66fa-4187-8f66-1ec5a1c862ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "net.load_state_dict(checkpoint['net'])\n",
    "start_epoch = checkpoint['epoch']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "81959d05-d08f-4cda-856a-7a2d9038df68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://raw.githubusercontent.com/brendenlake/omniglot/master/python/images_background.zip to /scratch/gg2501/ML_Project/data/omniglot-py/images_background.zip\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bdf92084a661476393d0d758eb598525",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9464212 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /scratch/gg2501/ML_Project/data/omniglot-py/images_background.zip to /scratch/gg2501/ML_Project/data/omniglot-py\n"
     ]
    }
   ],
   "source": [
    "omniglot_transforms = transforms.Compose([\n",
    "                      transforms.ToTensor(),\n",
    "                      transforms.Resize((32,32)),\n",
    "                      transform_to_three_channel\n",
    "                    ])\n",
    "\n",
    "omniglot_dataset = Omniglot(root=data_dir, background=True, download=True, transform=omniglot_transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "303f1320-a568-4d7f-b58d-0b396b7cb3be",
   "metadata": {},
   "outputs": [],
   "source": [
    "first, second = torch.utils.data.random_split(omniglot_dataset, [10000, 9280])\n",
    "omniglot_loader = torch.utils.data.DataLoader(first, batch_size=64, shuffle=True, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "997dcce5-56aa-4770-914f-20ec58a98c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_xs = []\n",
    "all_ys = []\n",
    "all_zs = []\n",
    "all_pred_labels = []\n",
    "\n",
    "net.eval()\n",
    "with torch.no_grad():\n",
    "    with tqdm(total=len(omniglot_loader.dataset)) as progress_bar:\n",
    "        for x, y in omniglot_loader:\n",
    "            all_xs.append(x.data.numpy())\n",
    "            all_ys.append(y.data.numpy())\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            z = net(x)\n",
    "            all_zs.append(z.cpu().data.numpy())\n",
    "            sldj = net.logdet()\n",
    "\n",
    "            preds = loss_fn.prior.classify(z.reshape((len(z), -1)))\n",
    "            preds = preds.reshape(y.shape)\n",
    "            all_pred_labels.append(preds.cpu().data.numpy())\n",
    "            acc = (preds == y).float().mean().item()\n",
    "            progress_bar.update(x.size(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9f4d34f-6068-4731-8cc1-bbf343afb0b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "(np.hstack(all_pred_labels) == 1).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38ad452a-fce9-4485-9b1f-ed2f84fcf344",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ds = SLDataset(test_config)\n",
    "test_ds.prepare(in_data='cifar', indata_size=1000, outdata_size=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a7c9c29-d7ee-4286-b951-10c85579aed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_loader = torch.utils.data.DataLoader(mnist_test_dataset, batch_size=64, shuffle=True, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e57adcc-c2b4-4cd8-8ef0-d4dcf90785f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_xs = []\n",
    "all_ys = []\n",
    "all_zs = []\n",
    "all_pred_labels = []\n",
    "\n",
    "net.eval()\n",
    "with torch.no_grad():\n",
    "    with tqdm(total=len(mnist_loader.dataset)) as progress_bar:\n",
    "        for x, y in mnist_loader:\n",
    "            all_xs.append(x.data.numpy())\n",
    "            all_ys.append(y.data.numpy())\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            z = net(x)\n",
    "            all_zs.append(z.cpu().data.numpy())\n",
    "            sldj = net.logdet()\n",
    "\n",
    "            preds = loss_fn.prior.classify(z.reshape((len(z), -1)))\n",
    "            preds = preds.reshape(y.shape)\n",
    "            all_pred_labels.append(preds.cpu().data.numpy())\n",
    "            acc = (preds == y).float().mean().item()\n",
    "            progress_bar.update(x.size(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92ecd823-cf62-4298-a8a0-647a8e6e3310",
   "metadata": {},
   "outputs": [],
   "source": [
    "(np.hstack(all_pred_labels) == all_ys).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cfc90ec-a35c-4de2-b621-161c3c1c0ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ys = np.zeros((10000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ae1d6b0-82e4-41cc-bfa6-87d5407bf75e",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35b3f19d-3273-4cfe-a34a-e384b655834e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import auc, precision_recall_curve, roc_curve\n",
    "\n",
    "def auroc(preds, labels, pos_label=1):\n",
    "    \"\"\"Calculate and return the area under the ROC curve using unthresholded predictions on the data and a binary true label.\n",
    "    \n",
    "    preds: array, shape = [n_samples]\n",
    "           Target normality scores, can either be probability estimates of the positive class, confidence values, or non-thresholded measure of decisions.\n",
    "           i.e.: an high value means sample predicted \"normal\", belonging to the positive class\n",
    "           \n",
    "    labels: array, shape = [n_samples]\n",
    "            True binary labels in range {0, 1} or {-1, 1}.\n",
    "    pos_label: label of the positive class (1 by default)\n",
    "    \"\"\"\n",
    "    fpr, tpr, _ = roc_curve(labels, preds, pos_label=pos_label)\n",
    "    return auc(fpr, tpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d98091c2-a849-44b9-b970-b6a67d241343",
   "metadata": {},
   "outputs": [],
   "source": [
    "auroc(np.hstack(all_pred_labels), all_ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b632b25-4866-4b44-a1cb-71484908bbf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestDataset():\n",
    "    def __init__(self, config: dict):\n",
    "        self.data_keys = set(['mnist', 'fashionmnist', 'cifar', 'svhn'])\n",
    "        self.config = config\n",
    "        self.image_tensors = []\n",
    "        self.labels = []\n",
    "    \n",
    "    def prepare(self, in_data='cifar', out_data=['svhn'], indata_size=600, outdata_size=600):\n",
    "        # Prepare OOD data\n",
    "        s = outdata_size // len(out_data)\n",
    "        for k in out_data:\n",
    "            dataset = config[k]['dataset']\n",
    "            transforms = config[k]['transforms']\n",
    "            for i, (img, _) in enumerate(dataset):\n",
    "                if i == outdata_size:\n",
    "                    break\n",
    "                img_tensor = transforms(img)\n",
    "                self.image_tensors.append(img_tensor)\n",
    "            self.labels += [0] * outdata_size\n",
    "        \n",
    "        # Prepare ID data\n",
    "        dataset = config[in_data]['dataset']\n",
    "        transforms = config[in_data]['transforms']\n",
    "        for i, (img, _) in enumerate(dataset):\n",
    "            if i == indata_size:\n",
    "                break\n",
    "            img_tensor = transforms(img)\n",
    "            self.image_tensors.append(img_tensor)\n",
    "        self.labels += [1] * indata_size\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_tensors)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.image_tensors[idx], self.labels[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb64df03-bdff-4634-9de8-a7e55ff5dc77",
   "metadata": {},
   "outputs": [],
   "source": [
    "testds = TestDataset(test_config)\n",
    "testds.prepare(indata_size=1000, outdata_size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3252226-825f-4cba-8272-912c6e07e084",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = torch.utils.data.DataLoader(testds, batch_size=64, shuffle=True, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b28ee2e-de63-4c37-84d3-ac85caba2218",
   "metadata": {},
   "outputs": [],
   "source": [
    "resume = './8.pt'\n",
    "print('Resuming from checkpoint at', resume)\n",
    "checkpoint = torch.load(resume)\n",
    "net.load_state_dict(checkpoint['net'])\n",
    "start_epoch = checkpoint['epoch']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22d505e3-234c-4ba0-b933-184b26861d9e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
