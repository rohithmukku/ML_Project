{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ed8347a9-f7a4-48f0-a865-46a7c433c6b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8b8a736c-f24f-4aec-8fba-c7495dc0b86a",
   "metadata": {},
   "outputs": [],
   "source": [
    "root = \"/scratch/rm5708/ml/ML_Project/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2fc182a2-8229-4577-a14e-bdc03c319fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(os.path.join(root, 'flowgmm-public'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "076dcdf3-13ad-4440-9baa-50095c92129f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/ext3/miniconda3/envs/full_time/lib/python3.8/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.1\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import flow_ssl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2c4640cb-7449-43e0-a370-8feaf2e65e8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yolo\n"
     ]
    }
   ],
   "source": [
    "print(\"yolo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c1216aac-ea5a-4df5-be24-acf78a446b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets import SVHN, MNIST, FashionMNIST, CIFAR10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "168123ec-107e-42e2-82b3-e22ae408ef4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = os.path.join(root, 'data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ff494df0-5a54-45ae-956a-5fec2b26f1fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using downloaded and verified file: /scratch/rm5708/ml/ML_Project/data/train_32x32.mat\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "svhn_dataset = SVHN(root=data_dir, split='train', download=True)\n",
    "mnist_dataset = MNIST(root=data_dir, download=True)\n",
    "fashionmnist_dataset = FashionMNIST(root=data_dir, download=True)\n",
    "cifar_dataset = CIFAR10(root=data_dir, download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2e8d1ce8-6872-4e6e-bfd8-2531f88343d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "import torch\n",
    "from torch.utils.data.sampler import Sampler\n",
    "import numpy as np\n",
    "import itertools\n",
    "import random\n",
    "\n",
    "class Dataset():\n",
    "    def __init__(self, config: dict):\n",
    "        self.data_keys = set(['mnist', 'fashionmnist', 'cifar', 'svhn'])\n",
    "        self.config = config\n",
    "        self.labeled_ids = []\n",
    "        self.unlabeled_ids = []\n",
    "        self.image_tensors = []\n",
    "        self.labels = []\n",
    "    \n",
    "    def prepare(self, in_data='mnist', indata_size=50000, outdata_size=18000, label_ratio=0.2):\n",
    "        print(self.data_keys)\n",
    "        self.data_keys.remove(in_data)\n",
    "        # Prepare OOD data\n",
    "        for k in self.data_keys:\n",
    "            dataset = config[k]['dataset']\n",
    "            transforms = config[k]['transforms']\n",
    "            start_id = len(self.labels)\n",
    "            end_id = start_id + int(0.2 * outdata_size)\n",
    "            for i, (img, _) in enumerate(dataset):\n",
    "                if i == outdata_size:\n",
    "                    break\n",
    "                img_tensor = transforms(img)\n",
    "                self.image_tensors.append(img_tensor)\n",
    "            self.labels += [0] * (int(0.2 * outdata_size))\n",
    "            self.labels += [-1] * (int(0.8 * outdata_size))\n",
    "            self.labeled_ids += range(start_id, end_id)\n",
    "            self.unlabeled_ids += range(end_id, len(self.labels))\n",
    "        \n",
    "        # Prepare ID data\n",
    "        dataset = config[in_data]['dataset']\n",
    "        transforms = config[in_data]['transforms']\n",
    "        start_id = len(self.labels)\n",
    "        end_id = start_id + int(0.2 * indata_size)\n",
    "        for i, (img, _) in enumerate(dataset):\n",
    "            if i == indata_size:\n",
    "                break\n",
    "            img_tensor = transforms(img)\n",
    "            self.image_tensors.append(img_tensor)\n",
    "        self.labels += [1] * (int(0.2 * indata_size))\n",
    "        self.labels += [-1] * (int(0.8 * indata_size))\n",
    "        self.labeled_ids += range(start_id, end_id)\n",
    "        self.unlabeled_ids += range(end_id, len(self.labels))\n",
    "        \n",
    "        random.shuffle(self.labeled_ids)\n",
    "        random.shuffle(self.unlabeled_ids)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_tensors)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        labeled_id = self.labeled_ids[idx % len(self.labeled_ids)]\n",
    "        unlabeled_id = self.unlabeled_ids[idx % len(self.unlabeled_ids)]\n",
    "        return self.image_tensors[labeled_id], self.image_tensors[unlabeled_id], self.labels[labeled_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "84b9a022-be6a-4b52-8ffb-a43303b35f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "svhn_transforms = transforms.Compose([\n",
    "                    transforms.Grayscale(),\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Resize((32,32))\n",
    "                ])\n",
    "\n",
    "mnist_transforms = transforms.Compose([\n",
    "                   transforms.ToTensor(),\n",
    "                   transforms.Resize((32,32))\n",
    "                ])\n",
    "\n",
    "fashionmnist_transforms = transforms.Compose([\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Resize((32,32))\n",
    "                ])\n",
    "\n",
    "cifar_transforms = transforms.Compose([\n",
    "                    transforms.Grayscale(),\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Resize((32,32))\n",
    "                ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f96446aa-9056-4afc-9d1a-e6a8af4eeb05",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {}\n",
    "\n",
    "config['svhn'] = {}\n",
    "config['svhn']['dataset'] = svhn_dataset\n",
    "config['svhn']['transforms'] = svhn_transforms\n",
    "\n",
    "config['mnist'] = {}\n",
    "config['mnist']['dataset'] = mnist_dataset\n",
    "config['mnist']['transforms'] = mnist_transforms\n",
    "\n",
    "config['fashionmnist'] = {}\n",
    "config['fashionmnist']['dataset'] = fashionmnist_dataset\n",
    "config['fashionmnist']['transforms'] = fashionmnist_transforms\n",
    "\n",
    "config['cifar'] = {}\n",
    "config['cifar']['dataset'] = cifar_dataset\n",
    "config['cifar']['transforms'] = cifar_transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "be1ffb39-b419-4c66-9100-0dfc81b4c611",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'cifar', 'mnist', 'fashionmnist', 'svhn'}\n"
     ]
    }
   ],
   "source": [
    "ds = Dataset(config)\n",
    "ds.prepare()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8e6bcfaf-974b-4f0a-9b23-ddef934cad66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using downloaded and verified file: /scratch/rm5708/ml/ML_Project/data/test_32x32.mat\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "svhn_test_dataset = SVHN(root=data_dir, split='test', download=True)\n",
    "mnist_test_dataset = MNIST(root=data_dir, train=False, download=True)\n",
    "fashionmnist_test_dataset = FashionMNIST(root=data_dir, train=False, download=True)\n",
    "cifar_test_dataset = CIFAR10(root=data_dir, train=False, download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c460804b-f229-49d3-b362-68948025a1e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_config = {}\n",
    "\n",
    "test_config['svhn'] = {}\n",
    "test_config['svhn']['dataset'] = svhn_test_dataset\n",
    "test_config['svhn']['transforms'] = svhn_transforms\n",
    "\n",
    "test_config['mnist'] = {}\n",
    "test_config['mnist']['dataset'] = mnist_test_dataset\n",
    "test_config['mnist']['transforms'] = mnist_transforms\n",
    "\n",
    "test_config['fashionmnist'] = {}\n",
    "test_config['fashionmnist']['dataset'] = fashionmnist_test_dataset\n",
    "test_config['fashionmnist']['transforms'] = fashionmnist_transforms\n",
    "\n",
    "test_config['cifar'] = {}\n",
    "test_config['cifar']['dataset'] = cifar_test_dataset\n",
    "test_config['cifar']['transforms'] = cifar_transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "da501a38-eef3-4564-add8-434a2fdbc2aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'cifar', 'mnist', 'fashionmnist', 'svhn'}\n"
     ]
    }
   ],
   "source": [
    "test_ds = Dataset(test_config)\n",
    "test_ds.prepare(indata_size=6000, outdata_size=2000, label_ratio=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "da1d6d54-c15e-45ca-9bed-ef89a893325e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LabeledUnlabeledBatchSampler(Sampler):\n",
    "    \"\"\"Minibatch index sampler for labeled and unlabeled indices. \n",
    "\n",
    "    An epoch is one pass through the labeled indices.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "            self, \n",
    "            labeled_idx, \n",
    "            unlabeled_idx, \n",
    "            labeled_batch_size, \n",
    "            unlabeled_batch_size):\n",
    "\n",
    "        self.labeled_idx = labeled_idx\n",
    "        self.unlabeled_idx = unlabeled_idx\n",
    "        self.unlabeled_batch_size = unlabeled_batch_size\n",
    "        self.labeled_batch_size = labeled_batch_size\n",
    "\n",
    "        assert len(self.labeled_idx) >= self.labeled_batch_size > 0\n",
    "        assert len(self.unlabeled_idx) >= self.unlabeled_batch_size > 0\n",
    "\n",
    "    @property\n",
    "    def num_labeled(self):\n",
    "        return len(self.labeled_idx)\n",
    "\n",
    "    def __iter__(self):\n",
    "        labeled_iter = iterate_once(self.labeled_idx)\n",
    "        unlabeled_iter = iterate_eternally(self.unlabeled_idx)\n",
    "        return (\n",
    "            labeled_batch + unlabeled_batch\n",
    "            for (labeled_batch, unlabeled_batch)\n",
    "            in  zip(batch_iterator(labeled_iter, self.labeled_batch_size),\n",
    "                    batch_iterator(unlabeled_iter, self.unlabeled_batch_size))\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labeled_idx) // self.labeled_batch_size\n",
    "\n",
    "\n",
    "def iterate_once(iterable):\n",
    "    return np.random.permutation(iterable)\n",
    "\n",
    "\n",
    "def iterate_eternally(indices):\n",
    "    def infinite_shuffles():\n",
    "        while True:\n",
    "            yield np.random.permutation(indices)\n",
    "    return itertools.chain.from_iterable(infinite_shuffles())\n",
    "\n",
    "\n",
    "def batch_iterator(iterable, n):\n",
    "    \"Collect data into fixed-length chunks or blocks\"\n",
    "    args = [iter(iterable)] * n\n",
    "    return zip(*args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3e9cfe1b-ef0d-4476-b809-1950aff62642",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batch_sampler = LabeledUnlabeledBatchSampler(ds.labeled_ids, ds.unlabeled_ids, 32, 32)\n",
    "test_batch_sampler = LabeledUnlabeledBatchSampler(test_ds.labeled_ids, test_ds.unlabeled_ids, 32, 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d4e1596f-c240-4ca7-8b61-18d10f29909f",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader = torch.utils.data.DataLoader(ds, batch_size=64, shuffle=True, pin_memory=True)\n",
    "testloader = torch.utils.data.DataLoader(test_ds, batch_size=64, shuffle=True, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4630d7a2-7e5a-4a69-9da8-3d0c9bfb42c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader = torch.utils.data.DataLoader(ds, batch_sampler=train_batch_sampler, pin_memory=True)\n",
    "testloader = torch.utils.data.DataLoader(test_ds, batch_sampler=test_batch_sampler, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f553a657-6dc7-49ba-b50a-05ae687af69a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1\n",
      "torch.Size([64, 1, 32, 32])\n",
      "torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "for batch in trainloader:\n",
    "    print(batch[0].get_device())\n",
    "    print(batch[1].shape)\n",
    "    print(batch[2].shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2124dd42-191d-454d-b2bb-51705c0466f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_shape = (1, 32, 32)\n",
    "flow = 'MNISTResidualFlow'\n",
    "model_cfg = getattr(flow_ssl, flow)\n",
    "net = model_cfg(in_channels=img_shape[0], num_classes=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "65002dfb-566c-4820-add7-8ffcf1bf0869",
   "metadata": {},
   "outputs": [],
   "source": [
    "if flow in [\"iCNN3d\", \"iResnetProper\",\"SmallResidualFlow\",\"ResidualFlow\",\"MNISTResidualFlow\"]:\n",
    "    net = net.flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f5920588-44f1-46f1-90ce-575b5529cc7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from experiments.train_flows.utils import train_utils\n",
    "\n",
    "means = 'random'\n",
    "means_r = 1.0\n",
    "cov_std = 1.0\n",
    "img_shape = (1, 32, 32)\n",
    "device = 'cuda'\n",
    "n_classes = 2\n",
    "\n",
    "net = net.to(device)\n",
    "r = means_r\n",
    "cov_std = torch.ones((n_classes)) * cov_std\n",
    "cov_std = cov_std.to(device)\n",
    "means = train_utils.get_means(means, num_means=n_classes, r=means_r, trainloader=trainloader, \n",
    "                        shape=img_shape, device=device, net=net)\n",
    "means_init = means.clone().detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9487a807-bb50-4650-b26f-e294679e7af0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Means: tensor([[-0.2195,  0.6484,  1.0082,  ...,  0.3635,  0.2085, -0.1414],\n",
      "        [-2.1674, -0.5625,  0.2136,  ..., -1.2087, -0.5734, -0.6175]],\n",
      "       device='cuda:0')\n",
      "Cov std: tensor([1., 1.], device='cuda:0')\n",
      "Pairwise dists: [[ 0.         44.54605713]\n",
      " [44.54605713  0.        ]]\n"
     ]
    }
   ],
   "source": [
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "print(\"Means:\", means)\n",
    "print(\"Cov std:\", cov_std)\n",
    "means_np = means.cpu().numpy()\n",
    "print(\"Pairwise dists:\", cdist(means_np, means_np))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "dd372806-894b-4d04-bac1-9266b46015fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using learnable means\n"
     ]
    }
   ],
   "source": [
    "from flow_ssl.distributions import SSLGaussMixture\n",
    "from flow_ssl import FlowLoss\n",
    "\n",
    "means_trainable = True\n",
    "covs_trainable = True\n",
    "weights_trainable = True\n",
    "\n",
    "if means_trainable:\n",
    "    print(\"Using learnable means\")\n",
    "    means = torch.tensor(means_np, requires_grad=True, device=device)\n",
    "\n",
    "prior = SSLGaussMixture(means, device=device)\n",
    "prior.weights.requires_grad = weights_trainable\n",
    "prior.inv_cov_stds.requires_grad = covs_trainable\n",
    "loss_fn = FlowLoss(prior)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "416efc52-ebbf-4011-8055-15feb3e81734",
   "metadata": {},
   "outputs": [],
   "source": [
    "from experiments.train_flows.utils import norm_util\n",
    "import torch.optim as optim\n",
    "\n",
    "param_groups = norm_util.get_param_groups(net, 0.0, norm_suffix='weight_g')\n",
    "\n",
    "optimizer = optim.Adam(param_groups, lr=1e-3)\n",
    "opt_gmm = optim.Adam([prior.means, prior.weights, prior.inv_cov_stds], lr=1e-4, weight_decay=0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "11c3f709-4854-4f51-95c1-9d0ab84a5ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "writer = SummaryWriter(log_dir='./')\n",
    "device = 'cuda' if torch.cuda.is_available() and len([0]) > 0 else 'cpu'\n",
    "start_epoch = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c5aebe9b-7dd6-4e51-8ae8-24ce144978cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_labels = len(ds.labeled_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c129e139-d9c4-48b9-8d47-cd070b329153",
   "metadata": {},
   "outputs": [],
   "source": [
    "from experiments.train_flows.utils import optim_util\n",
    "\n",
    "def train(epoch, net, trainloader, device, optimizer, opt_gmm, loss_fn,\n",
    "          label_weight, max_grad_norm, consistency_weight,\n",
    "          writer, use_unlab=True,  acc_train_all_labels=False,\n",
    "          ):\n",
    "    print('\\nEpoch: %d' % epoch)\n",
    "    net.train()\n",
    "    loss_meter = shell_util.AverageMeter()\n",
    "    loss_unsup_meter = shell_util.AverageMeter()\n",
    "    loss_nll_meter = shell_util.AverageMeter()\n",
    "    loss_consistency_meter = shell_util.AverageMeter()\n",
    "    jaclogdet_meter = shell_util.AverageMeter()\n",
    "    acc_meter = shell_util.AverageMeter()\n",
    "    acc_all_meter = shell_util.AverageMeter()\n",
    "    with tqdm(total=total_labels) as progress_bar:\n",
    "        for x1, x2, y in trainloader:\n",
    "\n",
    "            x1 = x1.to(device)\n",
    "            if not acc_train_all_labels:\n",
    "                y = y.to(device)\n",
    "            else:\n",
    "                y, y_all_lab = y[:, 0], y[:, 1]\n",
    "                y = y.to(device)\n",
    "                y_all_lab = y_all_lab.to(device)\n",
    "\n",
    "            labeled_mask = (y != NO_LABEL)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            opt_gmm.zero_grad()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                x2 = x2.to(device)\n",
    "                # print(x2.get_device(), next(net.parameters()).is_cuda)\n",
    "                z2 = net(x2)\n",
    "                z2 = z2.detach()\n",
    "                pred2 = loss_fn.prior.classify(z2.reshape((len(z2), -1)))\n",
    "\n",
    "            z1 = net(x1)\n",
    "            sldj = net.logdet()\n",
    "\n",
    "            z_all = z1.reshape((len(z1), -1))\n",
    "            z_labeled = z_all[labeled_mask]\n",
    "            y_labeled = y[labeled_mask]\n",
    "\n",
    "            logits_all = loss_fn.prior.class_logits(z_all)\n",
    "            logits_labeled = logits_all[labeled_mask]\n",
    "            loss_nll = F.cross_entropy(logits_labeled, y_labeled)\n",
    "\n",
    "            if use_unlab:\n",
    "                loss_unsup = loss_fn(z1, sldj=sldj)\n",
    "                loss = loss_nll * label_weight + loss_unsup\n",
    "            else:\n",
    "                loss_unsup = torch.tensor([0.])\n",
    "                loss = loss_nll\n",
    "\n",
    "            # consistency loss\n",
    "            loss_consistency = loss_fn(z1, sldj=sldj, y=pred2)\n",
    "            loss = loss + loss_consistency * consistency_weight\n",
    "\n",
    "            loss.backward()\n",
    "            optim_util.clip_grad_norm(optimizer, max_grad_norm)\n",
    "            optimizer.step()\n",
    "            opt_gmm.step()\n",
    "\n",
    "            preds_all = torch.argmax(logits_all, dim=1)\n",
    "            preds = preds_all[labeled_mask]\n",
    "            acc = (preds == y_labeled).float().mean().item()\n",
    "            if acc_train_all_labels:\n",
    "                acc_all = (preds_all == y_all_lab).float().mean().item()\n",
    "            else:\n",
    "                acc_all = acc\n",
    "\n",
    "            acc_meter.update(acc, x1.size(0))\n",
    "            acc_all_meter.update(acc_all, x1.size(0))\n",
    "            loss_meter.update(loss.item(), x1.size(0))\n",
    "            loss_unsup_meter.update(loss_unsup.item(), x1.size(0))\n",
    "            loss_nll_meter.update(loss_nll.item(), x1.size(0))\n",
    "            jaclogdet_meter.update(sldj.mean().item(), x1.size(0))\n",
    "            loss_consistency_meter.update(loss_consistency.item(), x1.size(0))\n",
    "\n",
    "            progress_bar.set_postfix(loss=loss_meter.avg,\n",
    "                                     bpd=optim_util.bits_per_dim(x1, loss_unsup_meter.avg),\n",
    "                                     acc=acc_meter.avg,\n",
    "                                     acc_all=acc_all_meter.avg)\n",
    "            progress_bar.update(y_labeled.size(0))\n",
    "\n",
    "    x1_img = torchvision.utils.make_grid(x1[:10], nrow=2 , padding=2, pad_value=255)\n",
    "    x2_img = torchvision.utils.make_grid(x2[:10], nrow=2 , padding=2, pad_value=255)\n",
    "    writer.add_image(\"data/x1\", x1_img)\n",
    "    writer.add_image(\"data/x2\", x2_img)\n",
    "\n",
    "    writer.add_scalar(\"train/loss\", loss_meter.avg, epoch)\n",
    "    writer.add_scalar(\"train/loss_unsup\", loss_unsup_meter.avg, epoch)\n",
    "    writer.add_scalar(\"train/loss_nll\", loss_nll_meter.avg, epoch)\n",
    "    writer.add_scalar(\"train/jaclogdet\", jaclogdet_meter.avg, epoch)\n",
    "    writer.add_scalar(\"train/acc\", acc_meter.avg, epoch)\n",
    "    writer.add_scalar(\"train/acc_all\", acc_all_meter.avg, epoch)\n",
    "    writer.add_scalar(\"train/bpd\", utils.bits_per_dim(x1, loss_unsup_meter.avg), epoch)\n",
    "    writer.add_scalar(\"train/loss_consistency\", loss_consistency_meter.avg, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60bb4663-d856-41b4-8e60-b657bba607bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 0\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.015316009521484375,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 43,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 20800,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f313e6332d784eeb9804f0313b03d335",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20800 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from experiments.train_flows.utils import shell_util\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "import torch.nn as nn\n",
    "import math\n",
    "import torch.nn.init as init\n",
    "import torch.nn.functional as F\n",
    "\n",
    "NO_LABEL = -1\n",
    "schedule = None\n",
    "n_epochs = 10\n",
    "lr = 5e-4\n",
    "lr_gmm = 1e-4\n",
    "consistency_weight = 1.0\n",
    "consistency_rampup = 1\n",
    "label_weight = 1.0\n",
    "max_grad_norm = 100.0\n",
    "save_freq = 2\n",
    "ckptdir = './'\n",
    "eval_freq = 1\n",
    "confusion = True\n",
    "num_samples = 50\n",
    "\n",
    "def linear_rampup(final_value, epoch, num_epochs, start_epoch=0):\n",
    "    t = (epoch - start_epoch + 1) / num_epochs\n",
    "    if t > 1:\n",
    "        t = 1.\n",
    "    return t * final_value\n",
    "\n",
    "for epoch in range(start_epoch, n_epochs):\n",
    "    cons_weight = linear_rampup(consistency_weight, epoch, consistency_rampup, start_epoch)\n",
    "    \n",
    "    writer.add_scalar(\"hypers/learning_rate\", lr, epoch)\n",
    "    writer.add_scalar(\"hypers/learning_rate_gmm\", lr_gmm, epoch)\n",
    "    writer.add_scalar(\"hypers/consistency_weight\", cons_weight, epoch)\n",
    "\n",
    "    train(epoch, net, trainloader, device, optimizer, opt_gmm, loss_fn,\n",
    "          label_weight, max_grad_norm, cons_weight,\n",
    "          writer, use_unlab=True)\n",
    "\n",
    "    # Save checkpoint\n",
    "    if (epoch % save_freq == 0):\n",
    "        print('Saving...')\n",
    "        state = {\n",
    "            'net': net.state_dict(),\n",
    "            'epoch': epoch,\n",
    "            'means': prior.means,\n",
    "        }\n",
    "        os.makedirs(ckptdir, exist_ok=True)\n",
    "        torch.save(state, os.path.join(ckptdir, str(epoch)+'.pt'))\n",
    "\n",
    "    # Save samples and data\n",
    "    if epoch % eval_freq == 0:\n",
    "        train_utils.test_classifier(epoch, net, testloader, device, loss_fn, writer, confusion=confusion)\n",
    "        # if args.swa:\n",
    "        #     optimizer.swap_swa_sgd() \n",
    "        #     print(\"updating bn\")\n",
    "        #     SWA.bn_update(bn_loader, net)\n",
    "        #     utils.test_classifier(epoch, net, testloader, device, loss_fn, \n",
    "        #             writer, postfix=\"_swa\")\n",
    "\n",
    "        z_means = prior.means\n",
    "        data_means = net.module.inverse(z_means)\n",
    "        z_mean_imgs = torchvision.utils.make_grid(\n",
    "                z_means.reshape((n_classes, *img_shape)), nrow=2)\n",
    "        data_mean_imgs = torchvision.utils.make_grid(\n",
    "                data_means.reshape((n_classes, *img_shape)), nrow=2)\n",
    "        writer.add_image(\"z_means\", z_mean_imgs, epoch)\n",
    "        writer.add_image(\"data_means\", data_mean_imgs, epoch)\n",
    "\n",
    "        means_np = prior.means.detach().cpu().numpy()\n",
    "        fig = plt.figure(figsize=(8, 8))\n",
    "        sns.heatmap(cdist(means_np, means_np))\n",
    "        img_data = torch.tensor(np.fromstring(fig.canvas.tostring_rgb(), dtype=np.uint8, sep=''))\n",
    "        img_data = torch.tensor(img_data.reshape(fig.canvas.get_width_height()[::-1] + (3,))).transpose(0, 2).transpose(1, 2)\n",
    "        writer.add_image(\"mean_dists\", img_data, epoch)\n",
    "\n",
    "        for i in range(n_classes):\n",
    "            writer.add_scalar(\"train_gmm/weight/{}\".format(i), F.softmax(prior.weights)[i], epoch)\n",
    "\n",
    "        for i in range(n_classes):\n",
    "            writer.add_scalar(\"train_gmm/cov/{}\".format(i), F.softplus(prior.inv_cov_stds[i])**2, epoch)\n",
    "\n",
    "        for i in range(n_classes):\n",
    "            writer.add_scalar(\"train_gmm/mean_dist_init/{}\".format(i), torch.norm(prior.means[i]-means_init[i], 2), epoch)\n",
    "\n",
    "        images = []\n",
    "        for i in range(n_classes):\n",
    "            images_cls = utils.sample(net, loss_fn.prior, num_samples // n_classes,\n",
    "                                      cls=i, device=device, sample_shape=img_shape)\n",
    "            images.append(images_cls)\n",
    "            images_cls_concat = torchvision.utils.make_grid(\n",
    "                    images_cls, nrow=2, padding=2, pad_value=255)\n",
    "            writer.add_image(\"samples/class_\"+str(i), images_cls_concat)\n",
    "        images = torch.cat(images)\n",
    "        os.makedirs(os.path.join(ckptdir, 'samples'), exist_ok=True)\n",
    "        images_concat = torchvision.utils.make_grid(images, nrow=num_samples //  n_classes , padding=2, pad_value=255)\n",
    "        os.makedirs(ckptdir, exist_ok=True)\n",
    "        torchvision.utils.save_image(images_concat, \n",
    "                                    os.path.join(ckptdir, 'samples/epoch_{}.png'.format(epoch)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47b9f598-4e1d-4679-a3c8-62c0614a7b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "net.logdet().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f16e7bce-a0e6-456c-9557-60e7ec274cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = iter(trainloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abe2d6ae-8c6c-4445-b89e-b0a1c659841e",
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = next(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac3c10c3-b581-4fa9-aee1-034c88ed754e",
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y, z = bs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52e7d542-849b-4ef3-ad87-2b76250acc1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = x.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f5893f1-e026-4370-a3a4-061824a7929c",
   "metadata": {},
   "outputs": [],
   "source": [
    "z = net(x)\n",
    "sldj = net.logdet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f58000f3-b17c-4de9-97b7-3096d54dd24e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "513b6656-b26b-42a0-9a96-f01c5cf6e169",
   "metadata": {},
   "outputs": [],
   "source": [
    "z = z.reshape((z.shape[0], -1))\n",
    "prior_ll = prior.log_prob(z)\n",
    "        # corrected_prior_ll = prior_ll - np.log(self.k) * np.prod(z.size()[1:]) \n",
    "        #PAVEL: why the correction?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2b6f4f5-3064-4efa-86fe-4e6a8ba51992",
   "metadata": {},
   "outputs": [],
   "source": [
    "prior_ll.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b0bb714-2629-4b09-a724-9dfbc0394826",
   "metadata": {},
   "outputs": [],
   "source": [
    "ll = prior_ll + sldj\n",
    "nll = -ll.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d070cc4f-a0d0-4073-918d-09023e336abf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "458eb79b-0ce5-4fdb-95ef-338493ff920b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SqueezeLayer(nn.Module):\n",
    "    def __init__(self, downscale_factor=2):\n",
    "        super().__init__()\n",
    "        self.downscale_factor = downscale_factor\n",
    "    def forward(self, x):\n",
    "        return squeeze(x,self.downscale_factor)\n",
    "    def inverse(self,y):\n",
    "        return unsqueeze(y,self.downscale_factor)\n",
    "    def logdet(self):\n",
    "        return 0\n",
    "\n",
    "def squeeze(input, downscale_factor=2):\n",
    "    '''\n",
    "    [:, C, H*r, W*r] -> [:, C*r^2, H, W]\n",
    "    '''\n",
    "    batch_size, in_channels, in_height, in_width = input.size()\n",
    "    out_channels = in_channels * (downscale_factor**2)\n",
    "\n",
    "    out_height = in_height // downscale_factor\n",
    "    out_width = in_width // downscale_factor\n",
    "\n",
    "    input_view = input.contiguous().view(\n",
    "        batch_size, in_channels, out_height, downscale_factor, out_width, downscale_factor\n",
    "    )\n",
    "\n",
    "    output = input_view.permute(0, 1, 3, 5, 2, 4).contiguous()\n",
    "    return output.view(batch_size, out_channels, out_height, out_width)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccea50c5-c64d-4401-a108-e46b4e2cd8e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def iResBlockConv(outer_channels,inner_channels):\n",
    "    gnet = nn.Sequential(\n",
    "        Swish(),\n",
    "        SpectralNormConv2d(outer_channels,inner_channels,3,padding=1,atol=0.001,rtol=0.001,coeff=0.98,stride=1),\n",
    "        Swish(),\n",
    "        SpectralNormConv2d(inner_channels,inner_channels,1,padding=0,atol=0.001,rtol=0.001,coeff=0.98,stride=1),\n",
    "        Swish(),\n",
    "        SpectralNormConv2d(inner_channels,outer_channels,3,padding=1,atol=0.001,rtol=0.001,coeff=0.98,stride=1))\n",
    "    return iSequential(iResBlock(gnet,n_dist='poisson'),ActNorm2d(outer_channels))\n",
    "\n",
    "def iResBlockLinear(outer_channels,inner_channels):\n",
    "    gnet = nn.Sequential(\n",
    "        Swish(),\n",
    "        SpectralNormLinear(outer_channels,inner_channels,atol=0.001,rtol=0.001,coeff=0.98),\n",
    "        Swish(),\n",
    "        SpectralNormLinear(inner_channels,inner_channels,atol=0.001,rtol=0.001,coeff=0.98),\n",
    "        Swish(),\n",
    "        SpectralNormLinear(inner_channels,outer_channels,atol=0.001,rtol=0.001,coeff=0.98))\n",
    "    return iSequential(iResBlock(gnet,n_dist='poisson'),ActNorm1d(outer_channels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5ea75e4-d048-4477-a210-676d409c0d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.init as init\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SpectralNormLinear(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "        self, in_features, out_features, bias=True, coeff=0.97, n_iterations=None, atol=None, rtol=None, **unused_kwargs\n",
    "    ):\n",
    "        del unused_kwargs\n",
    "        super(SpectralNormLinear, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.coeff = coeff\n",
    "        self.n_iterations = n_iterations\n",
    "        self.atol = atol\n",
    "        self.rtol = rtol\n",
    "        self.weight = nn.Parameter(torch.Tensor(out_features, in_features))\n",
    "        if bias:\n",
    "            self.bias = nn.Parameter(torch.Tensor(out_features))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "        self.reset_parameters()\n",
    "\n",
    "        h, w = self.weight.shape\n",
    "        self.register_buffer('scale', torch.tensor(0.))\n",
    "        self.register_buffer('u', F.normalize(self.weight.new_empty(h).normal_(0, 1), dim=0))\n",
    "        self.register_buffer('v', F.normalize(self.weight.new_empty(w).normal_(0, 1), dim=0))\n",
    "        self.compute_weight(True, 200)\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n",
    "        if self.bias is not None:\n",
    "            fan_in, _ = init._calculate_fan_in_and_fan_out(self.weight)\n",
    "            bound = 1 / math.sqrt(fan_in + 0.00001)\n",
    "            init.uniform_(self.bias, -bound, bound)\n",
    "\n",
    "    def compute_weight(self, update=True, n_iterations=None, atol=None, rtol=None):\n",
    "        n_iterations = self.n_iterations if n_iterations is None else n_iterations\n",
    "        atol = self.atol if atol is None else atol\n",
    "        rtol = self.rtol if rtol is None else atol\n",
    "\n",
    "        if n_iterations is None and (atol is None or rtol is None):\n",
    "            raise ValueError('Need one of n_iteration or (atol, rtol).')\n",
    "\n",
    "        if n_iterations is None:\n",
    "            n_iterations = 20000\n",
    "\n",
    "        u = self.u\n",
    "        v = self.v\n",
    "        weight = self.weight\n",
    "        if update:\n",
    "            with torch.no_grad():\n",
    "                itrs_used = 0.\n",
    "                for _ in range(n_iterations):\n",
    "                    old_v = v.clone()\n",
    "                    old_u = u.clone()\n",
    "                    # Spectral norm of weight equals to `u^T W v`, where `u` and `v`\n",
    "                    # are the first left and right singular vectors.\n",
    "                    # This power iteration produces approximations of `u` and `v`.\n",
    "                    v = F.normalize(torch.mv(weight.t(), u), dim=0, out=v)\n",
    "                    u = F.normalize(torch.mv(weight, v), dim=0, out=u)\n",
    "                    itrs_used = itrs_used + 1\n",
    "                    if atol is not None and rtol is not None and u.numel() != 0 and v.numel() != 0:\n",
    "                        err_u = torch.norm(u - old_u) / (u.nelement()**0.5)\n",
    "                        err_v = torch.norm(v - old_v) / (v.nelement()**0.5)\n",
    "                        tol_u = atol + rtol * torch.max(u)\n",
    "                        tol_v = atol + rtol * torch.max(v)\n",
    "                        if err_u < tol_u and err_v < tol_v:\n",
    "                            break\n",
    "                if itrs_used > 0:\n",
    "                    u = u.clone()\n",
    "                    v = v.clone()\n",
    "\n",
    "        sigma = torch.dot(u, torch.mv(weight, v))\n",
    "        with torch.no_grad():\n",
    "            self.scale.copy_(sigma)\n",
    "        # soft normalization: only when sigma larger than coeff\n",
    "        factor = torch.max(torch.ones(1).to(weight.device), sigma / self.coeff)\n",
    "        weight = weight / factor\n",
    "        return weight\n",
    "\n",
    "    def forward(self, input):\n",
    "        weight = self.compute_weight(update=self.training)\n",
    "        return F.linear(input, weight, self.bias)\n",
    "\n",
    "    def extra_repr(self):\n",
    "        return 'in_features={}, out_features={}, bias={}, coeff={}, n_iters={}, atol={}, rtol={}'.format(\n",
    "            self.in_features, self.out_features, self.bias is not None, self.coeff, self.n_iterations, self.atol,\n",
    "            self.rtol\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f34de0-96c2-4083-bcab-2b3d1253c0e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from flow_ssl.invertible import SqueezeLayer,padChannels,keepChannels,NNdownsample,iAvgPool2d,RandomPadChannels,Flatten\n",
    "#from torch.nn.utils import spectral_norm\n",
    "from flow_ssl.invertible import iLogits, iBN, MeanOnlyBN, iSequential, passThrough, addZslot, Join, pad_circular_nd,SN\n",
    "from flow_ssl.invertible import iConv2d, iResBlock\n",
    "from flow_ssl.conv_parts import ResBlock,conv2d\n",
    "from flow_ssl.icnn.icnn import FlowNetwork,StandardNormal\n",
    "from flow_ssl.invertible import Swish, ActNorm1d, ActNorm2d\n",
    "from flow_ssl.invertible.iresnet_trash import SpectralNormConv2d\n",
    "\n",
    "in_channels = 1\n",
    "num_per_block = 16\n",
    "k = 512\n",
    "\n",
    "flow = iSequential(\n",
    "        #iLogits(),\n",
    "        SqueezeLayer(),\n",
    "        *[iResBlockConv(in_channels*4,k) for i in range(num_per_block)],\n",
    "        SqueezeLayer(),\n",
    "        *[iResBlockConv(in_channels*16,k) for i in range(num_per_block)],\n",
    "        SqueezeLayer(),\n",
    "        *[iResBlockConv(in_channels*64,k) for i in range(num_per_block)],\n",
    "        Flatten(),\n",
    "        *[iResBlockLinear(1*32*32,k//4) for i in range(4)],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d652c36-8728-4ca7-b190-08261ae01114",
   "metadata": {},
   "outputs": [],
   "source": [
    "flow(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85e9c466-b4f6-4f86-a329-455280c72332",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
